# pipelines.py
import happybase
import hashlib
import json
import logging
import os
import jieba.analyse
from urllib.parse import unquote, urlparse
from scrapy.pipelines.files import FilesPipeline
from scrapy.utils.project import get_project_settings

# --- é˜¶æ®µä¸€ï¼šæ–‡ä»¶ä¸‹è½½ç®¡é“ ---
class MyFilesPipeline(FilesPipeline):
    """
    è´Ÿè´£å°† file_urls é‡Œçš„é“¾æ¥ä¸‹è½½åˆ°æœ¬åœ°ã€‚
    æ‰§è¡Œå®Œæ¯•åï¼Œä¼šå°†æœ¬åœ°è·¯å¾„å¡«å…¥ item['files']ã€‚
    """
    def file_path(self, request, response=None, info=None, *, item=None):
        # 1. è·å–é¡¹ç›®å (ç”¨äºåˆ›å»ºå­æ–‡ä»¶å¤¹)
        project_name = item.get('project', 'default')
        
        # 2. æå–åŸå§‹æ–‡ä»¶å
        url_path = urlparse(request.url).path
        decoded_path = unquote(url_path)
        filename = os.path.basename(decoded_path)
        
        # 3. å®¹é”™å¤„ç†
        if not filename:
            filename = hashlib.md5(request.url.encode()).hexdigest() + ".file"
            
        # æœ€ç»ˆä¿å­˜åœ¨: FILES_STORE/project_name/filename
        return f'{project_name}/{filename}'


# --- é˜¶æ®µäºŒï¼šå¤„ç†ä¸å…¥åº“ç®¡é“ ---
class HBasePipeline:
    """
    åŠŸèƒ½ï¼š
    1. è¿æ¥ HBase
    2. å¯¹æ–‡æœ¬è¿›è¡Œ TF-IDF å…³é”®è¯æå– (å«æƒé‡)
    3. å°†å…ƒæ•°æ®ã€å…³é”®è¯ã€æ–‡ä»¶è·¯å¾„å­˜å…¥ HBase
    """
    def __init__(self):
        self.settings = get_project_settings()
        self.host = self.settings.get('HBASE_HOST', '127.0.0.1')
        self.port = self.settings.getint('HBASE_PORT', 9090)
        self.table_name = self.settings.get('HBASE_TABLE', 'ustc_web_data')
        self.connection = None
        self.table = None

    def open_spider(self, spider):
        """çˆ¬è™«å¯åŠ¨æ—¶å»ºç«‹ HBase è¿æ¥"""
        try:
            # å¿…é¡»åŒ¹é… hbase thrift start -f -c (Framed Transport + Compact Protocol)
            self.connection = happybase.Connection(
                self.host, port=self.port, timeout=20000,
                transport='framed', protocol='compact'
            )
            self.connection.open()
            
            # è‡ªåŠ¨å»ºè¡¨é€»è¾‘
            tables = [t.decode('utf-8') for t in self.connection.tables()]
            if self.table_name not in tables:
                self.connection.create_table(
                    self.table_name,
                    {
                        'info': dict(),      # åŸºç¡€ä¿¡æ¯ (æ ‡é¢˜, URL, å…³é”®è¯)
                        'content': dict(),   # æ–‡æœ¬å†…å®¹
                        'files': dict()      # æ–‡ä»¶è·¯å¾„ä¿¡æ¯
                    }
                )
            self.table = self.connection.table(self.table_name)
            logging.info("âœ… [HBase] Pipeline Ready.")
        except Exception as e:
            logging.error(f"âŒ [HBase] Connection Failed: {e}")

    def close_spider(self, spider):
        if self.connection:
            self.connection.close()

    def process_item(self, item, spider):
        # å¦‚æœè¿æ¥æ²¡æˆåŠŸï¼Œç›´æ¥è¿”å›ï¼Œé˜²æ­¢æŠ¥é”™å´©æºƒ
        if not self.table:
            return item

        try:
            # === 1. æ•°æ®å‡†å¤‡ ===
            url = item['url']
            # RowKey è®¾è®¡ï¼šä½¿ç”¨ URL çš„ MD5ï¼Œç¡®ä¿å”¯ä¸€ä¸”é•¿åº¦å›ºå®š
            row_key = hashlib.md5(url.encode('utf-8')).hexdigest()
            raw_text = item.get('parsed_text', '').replace('\x00', '')
            
            # === 2. å…³é”®è¯æå–ä¸åˆ†æ (TF-IDF) ===
            # æå–å‰ 20 ä¸ªé«˜é¢‘è¯ï¼Œå¹¶ä¿ç•™æƒé‡ (withWeight=True)
            # æƒé‡å¯¹äºåç»­çš„â€œæ–‡æ¡£æ£€ç´¢å¼•æ“â€è®¡ç®—ç›¸å…³åº¦éå¸¸é‡è¦
            keywords_data = []
            if raw_text:
                tags = jieba.analyse.extract_tags(
                    raw_text, topK=20, withWeight=True, 
                    allowPOS=('n', 'nz', 'v', 'vd', 'vn', 'l', 'a', 'd') # ä»…æå–å®è¯
                )
                # è½¬æ¢ä¸º [{"word": "è®¡ç®—æœº", "weight": 1.23}, ...] æ ¼å¼
                keywords_data = [{"word": tag[0], "weight": tag[1]} for tag in tags]

            # === 3. è·å–æœ¬åœ°æ–‡ä»¶è·¯å¾„ ===
            local_file_paths = []
            if 'files' in item and item['files']:
                for f in item['files']:
                    local_file_paths.append(f['path'])

            # === 4. ç»„è£…æ•°æ® ===
            data = {
                b'info:url': url.encode('utf-8'),
                b'info:title': item['title'].encode('utf-8'),
                b'info:project': item['project'].encode('utf-8'),
                b'info:date': item.get('date', '').encode('utf-8'),
                
                # æ ¸å¿ƒä¿®æ”¹ï¼šå­˜å‚¨å¸¦æœ‰æƒé‡çš„å…³é”®è¯ JSON
                b'info:keywords': json.dumps(keywords_data, ensure_ascii=False).encode('utf-8'),
                
                # å­˜å…¥æ­£æ–‡ (æˆªå–å‰30000å­—ç¬¦é¿å…è¿‡å¤§)
                b'content:text': raw_text[:30000].encode('utf-8', 'ignore'),
                
                # å­˜å…¥æœ¬åœ°æ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨
                b'files:path': json.dumps(local_file_paths).encode('utf-8')
            }

            # === 5. å†™å…¥ HBase ===
            self.table.put(row_key, data)
            
            # æ—¥å¿—å±•ç¤º
            file_count = len(local_file_paths)
            # æ‰“å°å‰3ä¸ªå…³é”®è¯æ–¹ä¾¿è°ƒè¯•
            top_kw = ",".join([k['word'] for k in keywords_data[:3]])
            logging.info(f"ğŸ’¾ [Saved] {item['title'][:15]}... | Files: {file_count} | Keywords: {top_kw}")

        except Exception as e:
            logging.error(f"âŒ [Error] {e}")

        return item